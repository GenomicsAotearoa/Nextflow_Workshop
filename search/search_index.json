{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#reproducible-bioinformatics-workflows-with-nextflow-and-nf-core","title":"Reproducible Bioinformatics Workflows with Nextflow and nf-core","text":"<p>Nextflow is workflow management software that enables the writing of scalable and reproducible scientific workflows. It can integrate various software packages and environment management systems from environment modules to Docker, Singularity, and Conda. It allows for existing pipelines written in common scripting languages, such as R and Python, to be seamlessly coupled together. It implements a Domain Specific Language (DSL) that simplifies the implementation and running of workflows on cloud or high-performance computing (HPC) infrastructures.</p> <p>This lesson also introduces nf-core, a community-driven platform, which provides peer-reviewed best practice analysis pipelines written in Nextflow.</p> <p>Trainers</p> <ul> <li>Chris Hakkaart (Seqera Labs)</li> </ul> <p>Schedule</p>"},{"location":"#session-1","title":"Session 1","text":"<ul> <li>Introduction to Session 1</li> <li>Introduction to Nextflow</li> <li>Introduction to nf-core</li> <li>Configuring nf-core pipelines</li> <li>nf-core tools for users</li> </ul>"},{"location":"#session-2","title":"Session 2","text":"<ul> <li>Introduction to Session 2</li> <li>nf-core/sarek</li> <li>Configuring your run</li> <li>Configuring your deployment</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This workshop material was developed by Chris Hakkaart (Seqera Labs) with a huge amount of support from George Wiggins (University of Otago; Genetics Otago), Ben Halliday (University of Otago; Genetics Otago), and Dinindu Senanayake (NeSI; Genomics Aotearoa).</p> <p>The workshop content was inspired by material written for Foundational Nextflow Training and the Reproducible workflows with nf-core run by Sydney Informatics Hub, University of Sydney.</p>"},{"location":"session_1/0_kickoff/","title":"Introduction to Session 1","text":"<p>During Session 1 we will establish fundamental ideas and skills that are essential for using Nextflow and customizing the execution of a nf-core pipeline.</p> <p>We will start the session discussing the core features of Nextflow and learning the fundamental commands and options for executing pipelines. Next, we will learn the core features of nf-core and its tooling. This knowledge will be applied as we learn the structure of an nf-core pipeline and how customize its execution. Finally we will learn about nf-core tooling for users and how it can help you execute your pipeline.</p> <p>The ideas and skills you learn will be applied during Session 2 when you explore the source code of the <code>nf-core/sarek</code> pipeline and learn how to customize its execution. The workshop will finish by exploring the ways you can keep records of you runs and how to share them with others.</p>"},{"location":"session_1/0_kickoff/#create-a-new-working-directory","title":"Create a new working directory","text":"<p>It is good practice to organize projects into their own folders to make it easier to track and replicate experiments over time.</p> <p>Start by creating a new directory for all of today\u2019s activities and move into it:</p> <pre><code>mkdir ~/session1 &amp;&amp; cd $_\n</code></pre>"},{"location":"session_1/1_introduction/","title":"Introduction to Nextflow","text":"<p>Objectives</p> <ul> <li>Learn about the core features of Nextflow</li> <li>Learn Nextflow terminology</li> <li>Learn fundamental commands and options for executing pipelines</li> </ul>"},{"location":"session_1/1_introduction/#what-is-nextflow","title":"What is Nextflow?","text":"<p>Nextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational pipelines.</p> <p>It is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations.</p> <p>Nextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment based on the dataflow programming model.</p> <p>Nextflow\u2019s core features are:</p> <ul> <li>Pipeline portability and reproducibility</li> <li>Scalability of parallelization and deployment</li> <li>Integration of existing tools, systems, and industry standards</li> </ul> <p>Whether you are working with genomics data or other large and complex data sets, Nextflow can help you to streamline your pipeline and improve your productivity.</p>"},{"location":"session_1/1_introduction/#processes-and-channels","title":"Processes and Channels","text":"<p>In Nextflow, processes and channels are the fundamental building blocks of a pipeline.</p> <p></p> <p></p> <p></p> <p>A process is a unit of execution that represents a single computational step in a pipeline. It is defined as a block of code that typically performs one specific task. Each process will specify its input and outputs, as well as any directives and conditional statements required for its execution. Processes can be written in any language that can be executed from the command line, such as Bash, Python, Perl, or R.</p> <p>Processes in are executed independently (i.e., they do not share a common writable state) as tasks. Multiple tasks can run in parallel, allowing for efficient utilization of computing resources. Nextflow is a top down pipeline manager and will automatically manage data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied.</p> <p>A channel is an asynchronous first-in, first-out (FIFO) queue that is used to join processes together. Channels allow data to passed between processes and can be used to manage data, parallelize tasks, and structure pipelines. Any process can define one or more channels as an input and output. Ultimately the pipeline execution flow itself, is implicitly defined by the channel declarations.</p> <p>Importantly, processes can be parameterised to allow for flexibility in their behavior and to enable their reuse in and between pipelines. Parameters can be defined in the process declaration and can be passed to the process at runtime. Parameters can be used to specify the input and output files, as well as any other parameters required for the process to execute.</p>"},{"location":"session_1/1_introduction/#execution-abstraction","title":"Execution abstraction","text":"<p>While a process defines what command or script is executed, the executor determines how and where the script is executed.</p> <p>Nextflow provides an abstraction between the pipeline\u2019s functional logic and the underlying execution system. This abstraction allows users to define a pipeline once and execute it on different computing platforms without having to modify the pipeline definition.</p> <p></p> <p></p> <p></p> <p>If not specified, Nextflow will execute locally. Executing locally is useful for pipeline development and testing purposes. However, for real-world computational pipelines, a high-performance computing (HPC) or cloud platform is often required.</p> <p>Nextflow provides a variety of built-in execution options, such as local execution, HPC cluster execution, and cloud-based execution, and allows users to easily switch between these options using command-line arguments.</p> <p>You can find a full list of supported executors as well as how to configure them in the Nextflow docs.</p>"},{"location":"session_1/1_introduction/#nextflow-cli","title":"Nextflow CLI","text":"<p>Nextflow implements a declarative domain-specific language (DSL) that simplifies the writing of complex data analysis pipelines as an extension of a general-purpose programming language. As a concise DSL, Nextflow handles recurrent use cases while having the flexibility and power to handle corner cases.</p> <p>Nextflow is an extension of the Groovy programming language which, in turn, is a super-set of the Java programming language. Groovy can be thought of as \u201cPython for Java\u201d and simplifies the code.</p> <p>Nextflow provides a robust command line interface for the management and execution of pipelines. Nextflow can be used on any POSIX compatible system (Linux, OS X, etc). It requires Bash 3.2 (or later) and Java 11 (or later) to be installed.</p> <p>Nextflow is distributed as a self-installing package and does not require any special installation procedure.</p> <p>How to install Nextflow locally</p> <ol> <li>Download the executable package using either <code>wget -qO- https://get.nextflow.io | bash</code> or <code>curl -s https://get.nextflow.io | bash</code></li> <li>Make the binary executable on your system by running <code>chmod +x nextflow</code></li> <li>Move the nextflow file to a directory accessible by your <code>$PATH</code> variable, e.g, <code>mv nextflow ~/bin/</code></li> </ol>"},{"location":"session_1/1_introduction/#nextflow-options-and-commands","title":"Nextflow options and commands","text":"<p>Nextflow provides a robust command line interface for the management and execution of pipelines. The top-level interface consists of options and commands.</p> <p>You can list Nextflow options and commands with the <code>-h</code> option:</p> <pre><code>nextflow -h\n</code></pre> Output<pre><code>Usage: nextflow [options] COMMAND [arg...]\n\nOptions:\n  -C\n     Use the specified configuration file(s) overriding any defaults\n  -D\n     Set JVM properties\n  -bg\n     Execute nextflow in background\n  -c, -config\n     Add the specified file to configuration set\n  -config-ignore-includes\n     Disable the parsing of config includes\n  -d, -dockerize\n     Launch nextflow via Docker (experimental)\n  [truncated]\n\nCommands:\n  clean         Clean up project cache and work directories\n  clone         Clone a project into a folder\n  config        Print a project configuration\n  [truncated]\n</code></pre> <p>Options for a commands can also be viewed by appending the -help option to a Nextflow command.</p> <p>For example, options for the the run command can be viewed:</p> <pre><code>nextflow run -help\n</code></pre> Output<pre><code>Execute a pipeline project\nUsage: run [options] Project name or repository url\n  Options:\n    -E\n       Exports all current system environment\n       Default: false\n    -ansi-log\n       Enable/disable ANSI console logging\n    -bucket-dir\n       Remote bucket where intermediate result files are stored\n    -cache\n       Enable/disable processes caching\n    -disable-jobs-cancellation\n       Prevent the cancellation of child jobs on execution termination\n    -dsl1\n       Execute the workflow using DSL1 syntax\n       Default: false\n    -dsl2\n       Execute the workflow using DSL2 syntax\n       Default: false\n    -dump-channels\n       Dump channels for debugging purpose\n    -dump-hashes\n       Dump task hash keys for debugging purpose\n       Default: false\n    [truncated]\n</code></pre> <p>Exercise</p> <p>Find out which version of Nextflow you are using.</p> Solution <p>You can find out which version of Nextflow you are using by executing:</p> <pre><code>nextflow -version\n</code></pre> <p>You should see the following:</p> Output<pre><code>N E X T F L O W\nversion 23.04.4 build 5881\ncreated 25-09-2023 15:34 UTC (26-09-2023 04:34 NZDT)\ncite doi:10.1038/nbt.3820\nhttp://nextflow.io\n</code></pre>"},{"location":"session_1/1_introduction/#managing-your-environment","title":"Managing your environment","text":"<p>You can use environment variables to control the Nextflow runtime and the underlying Java virtual machine. These variables can be exported before running a pipeline and will be interpreted by Nextflow.</p> <p>For most users, Nextflow will work without setting any environment variables. However, to improve reproducibility and to optimise your resources, you will benefit from establishing some of these.</p> <p>For example, for consistency, it is good practice to pin the version of Nextflow you are using with the <code>NXF_VER</code> variable:</p> <pre><code>export NXF_VER=&lt;version number&gt;\n</code></pre> <p>Exercise</p> <p>Change the version of Nextflow you are using to <code>23.04.0</code> by exporting an environmental variable:</p> Solution <p>Export the singularity cache using the <code>NXF_VER</code> environmental variable:</p> <pre><code>export NXF_VER=23.04.0\n</code></pre> <p>Check that the <code>NXF_VER</code> has been applied:</p> <pre><code>nextflow -version\n</code></pre> <p>You should see nextflow update and print the following:</p> Output<pre><code>N E X T F L O W\nversion 23.04.0 build 5857\ncreated 01-04-2023 21:09 UTC (02-04-2023 09:09 NZDT)\ncite doi:10.1038/nbt.3820\nhttp://nextflow.io\n</code></pre> <p>Environmental variables on NeSI</p> <p>The behaviour of Nextflow environmental variables won't work as expected if using a NeSI Nextflow module.</p> <p>Similarly, if you are using a shared resource, you may also consider including paths to where software is stored and can be accessed using the <code>NXF_SINGULARITY_CACHEDIR</code> or the <code>NXF_CONDA_CACHEDIR</code> variables:</p> <pre><code>export NXF_SINGULARITY_CACHEDIR=&lt;custom/path/to/conda/cache&gt;\n</code></pre> <p>Exercise</p> <p>Export the folder <code>/nesi/nobackup/nesi02659/nextflow-workshop</code> as the folder where remote Singularity images are stored:</p> Solution <p>Export the singularity cache using the <code>NXF_SINGULARITY_CACHEDIR</code> environmental variable:</p> <pre><code>export NXF_SINGULARITY_CACHEDIR=/nesi/nobackup/nesi02659/nextflow-workshop\n</code></pre> <p>Check that the <code>NXF_SINGULARITY_CACHEDIR</code> has been exported:</p> <pre><code>echo $NXF_SINGULARITY_CACHEDIR\n</code></pre> <p>How to manage environmental variables</p> <p>You may want to include these, or other environmental variables, in your <code>.bashrc</code> file (or alternate) that is loaded when you log in so you don\u2019t need to export variables every session.</p> <p>A complete list of environmental variables can be found in the Nextflow docs.</p>"},{"location":"session_1/1_introduction/#executing-a-pipeline","title":"Executing a pipeline","text":"<p>Nextflow seamlessly integrates with code repositories such as GitHub. This feature allows you to manage your project code and use public Nextflow pipelines quickly, consistently, and transparently.</p> <p>The Nextflow <code>pull</code> command will download a pipeline from a hosting platform into your global cache <code>$HOME/.nextflow/assets</code> folder. </p> <p>If you are pulling a project hosted in a remote code repository, you can specify its qualified name or the repository URL.</p> <p>The qualified name is formed by two parts - the owner name and the repository name separated by a <code>/</code> character. For example, if a Nextflow project <code>bar</code> is hosted in a GitHub repository <code>foo</code> at the address <code>http://github.com/foo/bar</code>, it could be pulled using:</p> <pre><code>nextflow pull foo/bar\n</code></pre> <p>Or by using the complete URL:</p> <pre><code>nextflow pull http://github.com/foo/bar\n</code></pre> <p>Alternatively, the Nextflow <code>clone</code> command can be used to download a pipeline into a local directory of your choice:</p> <pre><code>nextflow clone foo/bar &lt;your/path&gt;\n</code></pre> <p>The Nextflow <code>run</code> command is used to initiate the execution of a pipeline:</p> <pre><code>nextflow run foo/bar\n</code></pre> <p>If you <code>run</code> a pipeline, it will look for a local file with the pipeline name you\u2019ve specified. If that file does not exist, it will look for a public repository with the same name on GitHub (unless otherwise specified). If it is found, Nextflow will automatically <code>pull</code> the pipeline to your global cache and execute it.</p> <p>Warning</p> <p>Be aware of what is already in your current working directory where you launch your pipeline. If your current working directory contains nextflow configuration files you may encounter unexpected results.</p> <p>Exercise</p> <p>Execute the <code>hello</code> pipeline directly from <code>nextflow-io</code> GitHub repository.</p> Solution <p>Use the <code>run</code> command to execute the nextflow-io/hello pipeline:</p> <pre><code>nextflow run nextflow-io/hello\n</code></pre> Output<pre><code>N E X T F L O W  ~  version 23.04.0\nPulling nextflow-io/hello ...\ndownloaded from https://github.com/nextflow-io/hello.git\nLaunching `https://github.com/nextflow-io/hello` [silly_sax] DSL2 - revision: 1d71f857bb [master]\nexecutor &gt;  local (4)\n[e6/2132d2] process &gt; sayHello (3) [100%] 4 of 4 \u2714\nHola world!\n\nBonjour world!\n\nCiao world!\n\nHello world!\n</code></pre> <p>More information about the Nextflow <code>run</code> command can be found in the Nextflow docs.</p>"},{"location":"session_1/1_introduction/#executing-a-revision","title":"Executing a revision","text":"<p>When a Nextflow pipeline is created or updated using GitHub (or another code repository), a new revision is created. Each revision is identified by a unique number, which can be used to track changes made to the pipeline and to ensure that the same version of the pipeline is used consistently across different runs.</p> <p>The Nextflow <code>info</code> command can be used to view pipeline properties, such as the project name, repository, local path, main script, and revisions. The <code>*</code> indicates which revision of the pipeline you have stickied and will be executed when using the <code>run</code> command.</p> <pre><code>nextflow info &lt;pipeline&gt;\n</code></pre> <p>It is recommended that you use the revision flag every time you execute a pipeline to ensure that the version is correct.</p> <p>To use a specific revision, you simply need to add it to the command line with the <code>--revision</code> or <code>-r</code> flag. For example, to run a pipeline with the <code>v1.0</code> revision, you would use the following:</p> <pre><code>nextflow run &lt;pipeline&gt; -r v1.0\n</code></pre> <p>Nextflow automatically provides built-in support for version control using Git. With this, users can easily manage and track changes made to a pipeline over time. A revision can be a git <code>branch</code>, <code>tag</code> or commit <code>SHA</code> number, and can be used interchangeably.</p> <p>Exercise</p> <p>Execute the <code>hello</code> pipeline directly from the <code>nextflow-io</code> GitHub using the <code>v1.1</code> revision tag.</p> Solution <p>Use the <code>nextflow run</code> command to execute the <code>nextflow-io/hello</code> pipeline with the <code>v1.1</code> revision tag:</p> <pre><code>nextflow run nextflow-io/hello -r v1.1\n</code></pre> Output<pre><code>N E X T F L O W  ~  version 23.04.0\nNOTE: Your local project version looks outdated - a different revision is available in the remote repository [3b355db864]\nNextflow DSL1 is no longer supported \u2014 Update your script to DSL2, or use Nextflow 22.10.x or earlier\n</code></pre> <p>Warning</p> <p>The warning shown above is expected as the <code>v1.1</code> pipeline revision was written using an older version of Nextflow that uses the depreciated <code>echo</code> method.</p> <p>As both Nextflow and pipelines are updated independently over time, pipelines and Nextflow functions can get out of sync. While most nf-core pipelines are now <code>dsl2</code> (the current way of writing pipelines), some are still written in <code>dsl1</code> and may require older version of Nextflow.</p> <p>You can use an older version of nextflow on the fly by adding adding the environmental variable to the start of the run command </p> <pre><code>NXF_VER=22.10.0 nextflow run nextflow-io/hello -r v1.1\n</code></pre> Output<pre><code>N E X T F L O W  ~  version 22.10.0\nNOTE: Your local project version looks outdated - a different revision is available in the remote repository [3b355db864]\nLaunching `https://github.com/nextflow-io/hello` [amazing_lovelace] DSL1 - revision: baba3959d7 [v1.1]\nWARN: The use of `echo` method has been deprecated\nexecutor &gt;  local (4)\n[e6/cfda06] process &gt; sayHello (4) [100%] 4 of 4 \u2714\nBojour world! (version 1.1)\n\nHello world! (version 1.1)\n\nCiao world! (version 1.1)\n\nHola world! (version 1.1)\n</code></pre>"},{"location":"session_1/1_introduction/#nextflow-log","title":"Nextflow log","text":"<p>It is important to keep a record of the commands you have run to generate your results. Nextflow helps with this by creating and storing metadata and logs about the run in hidden files and folders in your current directory (unless otherwise specified). This data can be used by Nextflow to generate reports. It can also be queried using the Nextflow <code>log</code> command:</p> <pre><code>nextflow log\n</code></pre> <p>The <code>log</code> command has multiple options to facilitate the queries and is especially useful while debugging a pipeline and inspecting execution metadata. You can view all of the possible <code>log</code> options with <code>-h</code> flag:</p> <pre><code>nextflow log -h\n</code></pre> <p>To query a specific execution you can use the <code>RUN NAME</code> or a <code>SESSION ID</code>:</p> <pre><code>nextflow log &lt;run name&gt;\n</code></pre> <p>To get more information, you can use the <code>-f</code> option with named fields. For example:</p> <pre><code>nextflow log &lt;run name&gt; -f process,hash,duration\n</code></pre> <p>There are many other fields you can query. You can view a full list of fields with the <code>-l</code> option:</p> <pre><code>nextflow log -l\n</code></pre> <p>Exercise</p> <p>Use the <code>log</code> command to view with <code>process</code>, <code>hash</code>, and <code>script</code> fields for your tasks from your most recent Nextflow execution.</p> Solution <p>Use the <code>log</code> command to get a list of you recent executions:</p> <pre><code>nextflow log\n</code></pre> Output<pre><code>TIMESTAMP               DURATION        RUN NAME                STATUS  REVISION ID     SESSION ID                              COMMAND                       \n2023-08-29 07:33:48     3.6s            stupefied_bernard       OK      1d71f857bb      f9e18b71-d689-4589-be34-8cd98c1aab2e    nextflow run nextflow-io/hello\n</code></pre> <p>Query the process, hash, and script using the <code>-f</code> option for the most recent run:</p> <pre><code>nextflow log stupefied_bernard -f process,hash,script\n</code></pre> Output<pre><code>sayHello        f3/8f827f\n    echo 'Hola world!'\n\nsayHello        b8/b66545\n    echo 'Ciao world!'\n\nsayHello        3c/498a68\n    echo 'Bonjour world!'\n\nsayHello        6e/8d5b1a\n    echo 'Hello world!'\n</code></pre>"},{"location":"session_1/1_introduction/#execution-cache-and-resume","title":"Execution cache and resume","text":"<p>Task execution caching is an essential feature of modern pipeline managers. Accordingly, Nextflow provides an automated caching mechanism for every execution.</p> <p>When using the Nextflow <code>-resume</code> option, successfully completed tasks from previous executions are skipped and the previously cached results are used in downstream tasks.</p> <p>Nextflow caching mechanism works by assigning a unique ID to each task. The task unique ID is generated as a 128-bit hash value composing the the complete file path, file size, and last modified timestamp. These ID's are used to create a separate execution directory where the tasks are executed and the outputs are stored. Nextflow will take care of the inputs and outputs in these folders for you.</p> <p>A multi-step pipeline is required to demonstrate cache and resume. The <code>christopher-hakkaart/nf-core-demo</code> pipeline was created with the nf-core <code>create</code> command and has the same structure as nf-core pipelines. It is a toy example with 3 processes:</p> <ol> <li><code>SAMPLESHEET_CHECK</code><ul> <li>Executes a custom python script to check the input sample sheet is valid.</li> </ul> </li> <li><code>FASTQC</code><ul> <li>Executes FastQC using the <code>.fastq.gz</code> files from the sample sheet as inputs.</li> </ul> </li> <li><code>MULTIQC</code><ul> <li>Executes MultiQC using the FastQC reports generated by the <code>FASTQC</code> process.</li> </ul> </li> </ol> <p>The <code>christopher-hakkaartnf-core-demo</code> is a very small nf-core pipeline. It uses real data and bioinformatics software and requires additional configuration to run successfully.</p> <p>To run this example you will need to include two profiles in your execution command. Profiles are sets of configuration options that can be accessed by Nextflow. Profiles will be explained in greater detail during the configuring nf-core pipelines section of the workshop.</p> <p>To run this pipeline, both the <code>test</code> profile and a software management profile (such as <code>singularity</code>) are required:</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main\n</code></pre> This run requires 6GB of memory <p>If your session was spawned with less than 8GB of memory, above run will fail with the following error  <pre><code>Caused by:\nProcess requirement exceeds available memory -- req: 6 GB; avail: 4 GB\n</code></pre></p> <p>The command line output will print something like this:</p> Output<pre><code>N E X T F L O W  ~  version 23.04.0\nLaunching `https://github.com/christopher-hakkaart/nf-core-demo` [voluminous_kay] DSL2 - revision: 17521af3a8 [main]\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/demo v1.0dev-g17521af\n------------------------------------------------------\nCore Nextflow options\n  revision                  : main\n  runName                   : voluminous_kay\n  containerEngine           : singularity\n  launchDir                 : /scale_wlg_persistent/filesets/home/chrishakk/session1\n  workDir                   : /scale_wlg_persistent/filesets/home/chrishakk/session1/work\n  projectDir                : /home/chrishakk/.nextflow/assets/christopher-hakkaart/nf-core-demo\n  userName                  : chrishakk\n  profile                   : test,singularity\n  configFiles               : /home/chrishakk/.nextflow/assets/christopher-hakkaart/nf-core-demo/nextflow.config\n\nInput/output options\n  input                     : https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv\n  outdir                    : results\n\nReference genome options\n  genome                    : R64-1-1\n  fasta                     : s3://ngi-igenomes/igenomes/Saccharomyces_cerevisiae/Ensembl/R64-1-1/Sequence/WholeGenomeFasta/genome.fa\n\nInstitutional config options\n  config_profile_name       : Test profile\n  config_profile_description: Minimal test dataset to check pipeline function\n\nMax job request options\n  max_cpus                  : 2\n  max_memory                : 6.GB\n  max_time                  : 6.h\n\nGeneric options\n  tracedir                  : null/pipeline_info\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\nIf you use nf-core/demo for your analysis please cite:\n\n* The nf-core framework\n  https://doi.org/10.1038/s41587-020-0439-x\n\n* Software dependencies\n  https://github.com/nf-core/demo/blob/master/CITATIONS.md\n------------------------------------------------------\nDownloading plugin nf-amazon@1.16.1\n[f2/e5eb26] process &gt; NFCORE_DEMO:DEMO:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet_test_illumina_amplicon.csv) [100%] 1 of 1 \u2714\n[bb/f98425] process &gt; NFCORE_DEMO:DEMO:FASTQC (SAMPLE1_PE_T1)                                                 [100%] 4 of 4 \u2714\n[dd/728742] process &gt; NFCORE_DEMO:DEMO:MULTIQC                                                                [100%] 1 of 1 \u2714\n-\nCompleted at: 29-Sep-2023 22:16:49\nDuration    : 2m 27s\nCPU hours   : (a few seconds)\nSucceeded   : 6\n</code></pre> <p>Executing this pipeline will create a <code>work</code> directory and a <code>results</code> directory with selected results files.</p> <p>In the output above, the hexadecimal numbers, such as <code>bb/f98425</code>, identify the unique task execution. These numbers are also the prefix of the <code>work</code> directories where each task is executed.</p> <p>You can inspect the files produced by a task by looking inside the <code>work</code> directory and using these numbers to find the task-specific execution path:</p> <p>The files that have been selected for publication in the <code>results</code> folder can also be explored:</p> <pre><code>ls results\n</code></pre> <p>If you look inside the <code>work</code> directory of a <code>FASTQC</code> task, you will find the files that were staged and created when this task was executed:</p> <p>The <code>FASTQC</code> process runs four times, executing in a different work directories for each set of inputs. Therefore, in the previous example, the work directory [bb/f98425] represents just one of the four sets of input data that was processed.</p> <p>To print all the relevant paths to the screen, use the <code>-ansi-log</code> option can be used when executing your pipeline:</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -ansi-log false\n</code></pre> <p>It's very likely you will execute a pipeline multiple times as you find the parameters that best suit your data. You can save a lot of spaces (and time) if you resume a pipeline from the last step that was completed successfully or unmodified.</p> <p>By adding the <code>-resume</code> option to your <code>run</code> command you can use the cache rather than re-running successful tasks:</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -resume\n</code></pre> <p>If you <code>run</code> the <code>christopher-hakkaart/nf-core-deme</code> pipeline again without making any changes you will see that the cache is used:</p> Output<pre><code>[truncated]\n[5f/07e477] process &gt; NFCORE_DEMO:DEMO:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet_test_illumina_amplicon.csv) [100%] 1 of 1, cached: 1 \u2714\n[b2/873706] process &gt; NFCORE_DEMO:DEMO:FASTQC (SAMPLE2_PE_T1)                                                 [100%] 4 of 4, cached: 4 \u2714\n[ca/e8e0a8] process &gt; NFCORE_DEMO:DEMO:MULTIQC                                                                [100%] 1 of 1, cached: 1 \u2714\n[truncated]\n</code></pre> <p>In practical terms, the pipeline is executed from the beginning. However, before launching the execution of a process, Nextflow uses the task unique ID to check if the work directory already exists and that it contains a valid command exit state with the expected output files. If this condition is satisfied, the task execution is skipped and previously computed results are used as the process results.</p> <p>Notably, the <code>-resume</code> functionality is very sensitive. Even touching a file in the work directory can invalidate the cache.</p> <p>Exercise</p> <p>Invalidate the cache by touching a <code>.fastq.gz</code> file in a <code>FASTQC</code> task work directory (you can use the <code>touch</code> command). Execute the pipeline again with the <code>-resume</code> option to show that the cache has been invalidated.</p> Solution <p>Execute the pipeline for the first time (if you have not already).</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main\n</code></pre> <p>Use the task ID shown for the <code>FASTQC</code> process and use it to find and <code>touch</code> the <code>sample1_R1.fastq.gz</code> file:</p> <pre><code>touch work/b2/87370687cc7cdec037ce4f36807d32/sample1_R1.fastq.gz\n</code></pre> <p>Execute the pipeline again with the <code>-resume</code> command option:</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -resume\n</code></pre> <p>You should that 2 of 4 tasks for <code>FASTQC</code> and the <code>MULTIQC</code> task were invalid and were executed again.</p> <p>Why did this happen?</p> <p>In this example, the cache of two <code>FASTQC</code> tasks were invalid. The <code>sample1_R1.fastq.gz</code> file is used by in the samplesheet twice. Thus, touching the symlink for this file and changing the date of last modification disrupted two task executions.</p> <p>Your work directory can get very big very quickly (especially if you are using full sized datasets). It is good practise to <code>clean</code> your work directory regularly. Rather than removing the <code>work</code> folder with all of it's contents, the Nextflow <code>clean</code> function allows you to selectively remove data associated with specific runs.</p> <pre><code>nextflow clean -help\n</code></pre> <p>The <code>-after</code>, <code>-before</code>, and <code>-but</code> options are all very useful to select specific runs to <code>clean</code>. The <code>-dry-run</code> option is also very useful to see which files will be removed if you were to <code>-force</code> the <code>clean</code> command.</p> <p>Exercise</p> <p>You Nextflow to <code>clean</code> your work <code>work</code> directory of staged files but keep your execution logs.</p> Solution <p>Use the Nextflow <code>clean</code> command with the <code>-k</code> and <code>-f</code> options:</p> <pre><code>nextflow clean -k -f\n</code></pre>"},{"location":"session_1/1_introduction/#listing-and-dropping-cached-pipelines","title":"Listing and dropping cached pipelines","text":"<p>Over time, you might want to remove a stored pipelines. Nextflow also has functionality to help you to view and remove pipelines that have been pulled locally.</p> <p>The Nextflow <code>list</code> command prints the projects stored in your global cache folder (<code>$HOME/.nextflow/assets</code>). These are the pipelines that were pulled when you executed either of the Nextflow <code>pull</code> or <code>run</code> commands:</p> <pre><code>nextflow list\n</code></pre> <p>If you want to remove a pipeline from your cache you can remove it using the Nextflow <code>drop</code> command:</p> <pre><code>nextflow drop &lt; pipeline&gt;\n</code></pre> <p>Exercise</p> <p>View your cached pipelines with the Nextflow <code>list</code> command and remove the <code>nextflow-io/hello</code> pipeline with the <code>drop</code> command.</p> Solution <p>List your pipeline assets:</p> <pre><code>nextflow list\n</code></pre> <p>Drop the <code>nextflow-io/hello</code> pipeline:</p> <pre><code>nextflow drop nextflow-io/hello\n</code></pre> <p>Check it has been removed:</p> <pre><code>nextflow list\n</code></pre> <p></p> <p>Key points</p> <ul> <li>Nextflow is a pipeline orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational pipelines</li> <li>Environment variables can be used to control your Nextflow runtime and the underlying Java virtual machine</li> <li>Nextflow supports version control and has automatic integrations with online code repositories.</li> <li>Nextflow will cache your runs and they can be resumed with the <code>-resume</code> option</li> <li>You can manage pipelines with Nextflow commands (e.g., <code>pull</code>, <code>clone</code>, <code>list</code>, and <code>drop</code>)</li> </ul>"},{"location":"session_1/2_nfcore/","title":"Introduction to nf-core","text":"<p>Objectives</p> <ul> <li>Learn about the core features of nf-core.</li> <li>Learn how to use nf-core tooling.</li> <li>Use Nextflow to pull the <code>nf-core/sarek</code> workflow</li> </ul>"},{"location":"session_1/2_nfcore/#what-is-nf-core","title":"What is nf-core?","text":"<p>nf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.</p> <p>nf-core provides a standardised set of best practices, guidelines, and templates for building and sharing bioinformatics workflows. These workflows are designed to be modular, scalable, and portable, allowing researchers to easily adapt and execute them using their own data and compute resources.</p> <p>The community is a diverse group of bioinformaticians, developers, and researchers from around the world who collaborate on developing and maintaining a growing collection of high-quality workflows. These workflows cover a range of applications, including transcriptomics, proteomics, and metagenomics.</p> <p>One of the key benefits of nf-core is that it promotes open development, testing, and peer review, ensuring that the workflows are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries.</p> <p>nf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276\u2013278 (2020). Nature Biotechnology</p> <p>Key Features of nf-core workflows</p> <ul> <li>Documentation<ul> <li>nf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won't be left in the dark.</li> </ul> </li> <li>CI Testing<ul> <li>Every time a change is made to the workflow code, nf-core workflows use continuous-integration testing to ensure that nothing has broken.</li> </ul> </li> <li>Stable Releases<ul> <li>nf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible.</li> </ul> </li> <li>Packaged software<ul> <li>Pipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations.</li> </ul> </li> <li>Portable and reproducible<ul> <li>nf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to execute.</li> </ul> </li> <li>Cloud-ready<ul> <li>nf-core workflows are tested on AWS after every major release. You can even browse results live on the website and use outputs for your own benchmarking.</li> </ul> </li> </ul> <p>It is important to remember all nf-core workflows are open-source and community driven. Pipelines are under active community development and are regularly updated with fixes and other improvements. Even though the pipelines and tools undergo repeated community review and testing - it is important to check your results.</p>"},{"location":"session_1/2_nfcore/#events","title":"Events","text":"<p>nf-core events are community-driven gatherings that provide a platform to discuss the latest developments in Nextflow and nf-core workflows. These events include community seminars, trainings, and hackathons, and are open to anyone who is interested in using and developing nf-core and its applications. Most events are held virtually, making them accessible to a global audience.</p> <p>Upcoming events are listed on the nf-core event page and announced on Slack and Twitter.</p>"},{"location":"session_1/2_nfcore/#join-the-community","title":"Join the community!","text":"<p>There are several ways you can join the nf-core community. You are welcome to join any or all of these at any time!</p> <p> <p>The nf-core Slack is one of the primary resources for nf-core users. There are dedicated channels for all workflows as well as channels for common topics.</p> <p>If you are unsure of where to ask you questions - the <code>#help</code> and <code>#nostupidquestions</code> channels are a great place to start.</p> <p>Questions about Nextflow</p> <p>If you have questions about Nextflow and deployments that are not related to nf-core you can ask them on the Nextflow Slack. It's worthwhile joining both Slack groups and browsing the channels to get an idea of what types of questions are being asked on each channel. Searching channels can also be a great source of information as your question may have been asked before.</p> <p>Joining multiple nf-core and Nextflow channels is important to keep up to date with the latest community developments and updates. In particular, following the nf-core and Nextflow Twitter accounts will keep you up-to-date with community announcements. If you are looking for more information about a workflow, the nf-core YouTube channel regularly shares ByteSize seminars about best practises, workflows, and community developments.</p> <p>Exercise</p> <p>Join the nf-core Slack and fill in your profile information. If you're joining the nf-core Slack for the first time make sure you drop a message in <code>#say-hello</code> to introduce yourself! \ud83d\udc4b</p> Solution <p>Follow this link to join the nf-core Slack. Follow the instructions to enter your credentials and update your profile. Even if you are already a member of the nf-core Slack, it's a great time to check your profile is up-to-date.</p>"},{"location":"session_1/2_nfcore/#nf-core-tools","title":"nf-core tools","text":"<p>This workshop will make use of nf-core tools, a set of helper tools for use with Nextflow workflows. These tools have been developed to provide a range of additional functionality for using, developing, and testing workflows.</p> How to download nf-core tools - Don't have to do this install today as it is already installed to your workshop environment. <p>nf-core tools is written in Python and is available from the Python Package Index (PyPI):</p> <pre><code>pip install nf-core\n</code></pre> <p>Alternatively, nf-core tools can be installed from Bioconda:</p> <pre><code>conda install -c bioconda nf-core\n</code></pre> <p>The nf-core <code>--version</code> option can be used to print your version of nf-core tools:</p> <pre><code>nf-core --version\n</code></pre> <p>Exercise</p> <p>Find out what version of nf-core tools you have available using the nf-core <code>--version</code> option. If nf-core tools is not installed then install it using the commands above:</p> Solution <p>Use the nf-core <code>--version</code> option to print your nf-core tools version:</p> <pre><code>nf-core --version\n</code></pre> <p>If you get the message \"<code>nf-core: command not found</code>\" - install nf-core using the commands above:</p> <p>Download <code>nf-core</code> from the Python Package Index (PyPI):</p> <pre><code>pip install nf-core\n</code></pre> <p>Use the nf-core <code>--version</code> option to print your nf-core tools version:</p> <pre><code>nf-core --version\n</code></pre> <p>Warning</p> <p>Some of these commands may change depending on the operating system you are using.</p> <p>nf-core tools are for everyone and has commands to help both users and developers.</p> <p>For users, the tools make it easier to execute workflows.</p> <p>For developers, the tools make it easier to develop and test your workflows using best practices. You can read about the nf-core commands on the tools page of the nf-core website or using the command line.</p> <p>Exercise</p> <p>Find out what nf-core tools commands and options are available using the <code>--help</code> option:</p> Solution <p>Execute the <code>--help</code> option to list the options, commands for users, and commands for developers:</p> <pre><code>nf-core --help\n</code></pre> <p>nf-core tools is updated with new features and fixes regularly so it's best to keep your version of nf-core tools up-to-date.</p>"},{"location":"session_1/2_nfcore/#executing-an-nf-core-workflow","title":"Executing an nf-core workflow","text":"<p>There are currently 88 workflows (September 2023) available as part of nf-core. These workflows are at various stages of development with 53 released, 23 under development, and 12 archived.</p> <p>The nf-core website has a full list of workflows, as well as their documentation, which can be explored.</p> <p>Each workflow has a dedicated page that includes expansive documentation that is split into 6 sections:</p> <ul> <li>Introduction: An introduction and overview of the workflow</li> <li>Usage: Descriptions of how to execute the workflow</li> <li>Parameters: Grouped workflow parameters with descriptions</li> <li>Output: Descriptions and examples of the expected output files</li> <li>Results: Example output files generated from the full test dataset</li> <li>Releases &amp; Statistics: Workflow version history and statistics</li> </ul> <p>Unless you are actively developing workflow code, you don't need to clone the workflow code from GitHub and can use Nextflow\u2019s built-in functionality to <code>pull</code> and a workflow. As shown in the previous lesson, the Nextflow <code>pull</code> command can download and cache workflows from GitHub repositories:</p> <pre><code>nextflow pull nf-core/&lt;pipeline&gt;\n</code></pre> <p>Nextflow <code>run</code> will also automatically <code>pull</code> the workflow if it was not already available locally:</p> <pre><code>nextflow run nf-core/&lt;pipeline&gt;\n</code></pre> <p>Nextflow will <code>pull</code> the default git branch if a workflow version is not specified. This will be the master branch for nf-core workflows with a stable release. nf-core workflows use GitHub releases to tag stable versions of the code and software. You will always be able to execute a previous version of a workflow once it is released using the <code>-revision</code> or <code>-r</code> flag.</p> <p>Exercise</p> <p>Use Nextflow to <code>pull</code> the latest version of the <code>nf-core/sarek</code> workflow directly from GitHub:</p> Solution <p>Use Nextlfow to <code>pull</code> the <code>sarek</code> workflow from the <code>nf-core</code> GitHub repository:</p> <pre><code>nextflow pull nf-core/sarek -r 3.2.3\n</code></pre> <p>Check that it has been pulled by listing your cached pipelines:</p> <pre><code>nextflow list\n</code></pre> <p></p> <p>Key points</p> <ul> <li>nf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.</li> <li>You can join/follow nf-core on multiple different social channels (Slack, YouTube, Twitter...)</li> <li>nf-core has its own tooling that can be used by users and developers.</li> <li>Nextflow can be used to <code>pull</code> nf-core workflows.</li> </ul>"},{"location":"session_1/3_configuration/","title":"Configuring nf-core pipelines","text":"<p>Objectives</p> <ul> <li>Learn about the structure of an nf-core pipeline.</li> <li>Learn how to customize the execution of an nf-core pipeline.</li> <li>Customize a toy example of an nf-core pipeline.</li> </ul>"},{"location":"session_1/3_configuration/#pipeline-structure","title":"Pipeline structure","text":"<p>nf-core pipelines follow a set of best practices and standardised conventions. nf-core pipelines start from a common template and follow the same structure. Although you won\u2019t need to edit code in the pipeline project directory, having a basic understanding of the project structure and some core terminology will help you understand how to configure its execution.</p> <p>Nextflow DSL2 workflow are built up of subworkflows and modules that are stored as separate <code>.nf</code> files.</p> <p></p> <p></p> <p></p> <p>Most nf-core pipelines consist of a single workflow file (there are a few exceptions). This is the main <code>&lt;workflow&gt;.nf</code> file that is used to bring everything else together. Instead of having one large monolithic script, it is broken up into a combination of subworkflows and modules.</p> <p></p> <p></p> <p></p> <p>A subworkflows is a groups of modules that are used in combination with each other and have a common purpose. For example, the <code>SAMTOOLS_STATS</code>, <code>SAMTOOLS_IDXSTATS</code>, and <code>SAMTOOLS_FLAGSTAT</code> modules are all included in the <code>BAM_STATS_SAMTOOLS</code> subworkflow.</p> <p>Subworkflows improve pipeline readability and help with the reuse of modules within a pipeline. Within a nf-core pipeline, a subworkflow can be an nf-core subworkflow or as a local subworkflow. Like an nf-core pipeline, an nf-core subworkflow is developed by the community is shared in the nf-core subworkflows GitHub repository. Local subworkflows are pipeline specific that are not shared in the nf-core subworkflows repository.</p> <p>A modules is a wrapper for a process, the basic processing primitive to execute a user script. It can specify directives, inputs, outputs, when statements, and a script block.</p> <p>Most modules will execute a single tool in the script block and will make use of the directives, inputs, outputs, and when statements dynamically. Like subworkflows, modules can also be developed and shared in the nf-core modules GitHub repository or stored as a local module. All modules from the nf-core repository are version controlled and tested to ensure reproducibility. Local modules are pipeline specific that are not shared in the nf-core modules repository.</p>"},{"location":"session_1/3_configuration/#configuration","title":"Configuration","text":"<p>Each nf-core pipeline comes with a set of \u201csensible defaults\u201d. While the defaults are a great place to start, you will almost certainly want to modify these to fit your own purposes and system requirements.</p> <p>You do not need to edit the pipeline code to configure nf-core pipelines.</p> <p>When a pipeline is launched, Nextflow will look for configuration files in several locations. As each source can contain conflicting settings, the sources are ranked to decide which settings to apply. Configuration sources are reported below and listed in order of priority:</p> <ol> <li>Parameters specified on the command line (<code>--parameter</code>)</li> <li>Parameters that are provided using the <code>-params-file</code> option</li> <li>Config file that are provided using the <code>-c</code> option</li> <li>The config file named <code>nextflow.config</code> in the current directory</li> <li>The config file named <code>nextflow.config</code> in the pipeline project directory</li> <li>The config file <code>$HOME/.nextflow/config</code></li> <li>Values defined within the pipeline script itself (e.g., <code>main.nf</code>)</li> </ol> <p>Warning</p> <p>nf-core pipeline parameters must be passed via the command line (<code>--&lt;parameter&gt;</code>) or Nextflow <code>-params-file</code> option. Custom config files, including those provided by the <code>-c</code> option, can be used to provide any configuration except for parameters.</p> <p>Notably, while some of these files are already included in the nf-core pipeline repository (e.g., the <code>nextflow.config</code> file in the nf-core pipeline repository), some are automatically identified on your local system (e.g., the <code>nextflow.config</code> in the launch directory), and others are only included if they are specified using <code>run</code> options (e.g., <code>-params-file</code>, and <code>-c</code>).</p> <p>Understanding how and when these files are interpreted by Nextflow is critical for the accurate configuration of a pipelines execution.</p>"},{"location":"session_1/3_configuration/#parameters","title":"Parameters","text":"<p>Parameters are pipeline specific settings that can be used to customise the execution of a pipeline. </p> <p>Every nf-core pipeline has a full list of parameters on the nf-core website. When viewing these parameters online, you will also be shown a description and the type of the parameter. Some parameters will have additional text to help you understand when and how a parameter should be used.</p> <p>Parameters and their descriptions can also be viewed in the command line using the <code>run</code> command with the <code>--help</code> parameter:</p> <pre><code>nextflow run nf-core/&lt;workflow&gt; --help\n</code></pre> <p>Exercise</p> <p>View the parameters for the <code>christopher-hakkaart/nf-core-demo</code> pipeline using the command line:</p> Solution <p>The <code>christopher-hakkaart/nf-core-demo</code> pipeline parameters can be printed using the <code>run</code> command and the <code>--help</code> option:</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -r main --help\n</code></pre>"},{"location":"session_1/3_configuration/#parameters-in-the-command-line","title":"Parameters in the command line","text":"<p>At the highest level, parameters can be customised using the command line. Any parameter can be configured on the command line by prefixing the parameter name with a double dash (<code>--</code>):</p> <pre><code>nextflow nf-core/&lt;workflow&gt; --&lt;parameter&gt;\n</code></pre> <p>When to use <code>--</code> and <code>-</code></p> <p>Nextflow options are prefixed with a single dash (<code>-</code>) and pipeline parameters are prefixed with a double dash (<code>--</code>).</p> <p>Depending on the parameter type, you may be required to add additional information after your parameter flag. For example, for a string parameter, you would add the string after the parameter flag:</p> <pre><code>nextflow nf-core/&lt;workflow&gt; --&lt;parameter&gt; string\n</code></pre> <p>Exercise</p> <p>Give the MultiQC report for the <code>christopher-hakkaart/nf-core-demo</code> pipeline the name of your favorite animal using the <code>multiqc_title</code> parameter using a command line flag:</p> Solution <p>Add the <code>--multiqc_title</code> flag to your command and execute it. Use the <code>-resume</code> option to save time:</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main --multiqc_title kiwi -resume\n</code></pre> <p>In this example, you can check your parameter has been applied by listing the files created in the results folder (<code>results</code>):</p> <pre><code>ls results/multiqc/\n</code></pre> <p><code>--multiqc_title</code> is a parameter that directly impacts a result file. For parameters that are not as obvious, you may need to check your <code>log</code> to ensure your changes have been applied. You should not rely on the changes to parameters printed to the command line when you execute your run:</p> <pre><code>nextflow log\nnextflow log &lt;run name&gt; -f \"process,script\"\n</code></pre>"},{"location":"session_1/3_configuration/#default-configuration-files","title":"Default configuration files","text":"<p>All parameters will have a default setting that is defined using the <code>nextflow.config</code> file in the pipeline project directory. By default, most parameters are set to <code>null</code> or <code>false</code> and are only activated by a profile or configuration file.</p> <p>There are also several <code>includeConfig</code> statements in the <code>nextflow.config</code> file that are used to include additional <code>.config</code> files from the <code>conf/</code> folder. Each additional <code>.config</code> file contains categorised configuration information for your pipeline execution, some of which can be optionally included:</p> <ul> <li><code>base.config</code><ul> <li>Included by the pipeline by default.</li> <li>Generous resource allocations using labels.</li> <li>Does not specify any method for software management and expects software to be available (or specified elsewhere).</li> </ul> </li> <li><code>igenomes.config</code><ul> <li>Included by the pipeline by default.</li> <li>Default configuration to access reference files stored on AWS iGenomes.</li> </ul> </li> <li><code>modules.config</code><ul> <li>Included by the pipeline by default.</li> <li>Module-specific configuration options (both mandatory and optional).</li> </ul> </li> <li><code>test.config</code><ul> <li>Only included if specified as a profile.</li> <li>A configuration profile to test the pipeline with a small test dataset.</li> </ul> </li> <li><code>test_full.config</code><ul> <li>Only included if specified as a profile.</li> <li>A configuration profile to test the pipeline with a full-size test dataset.</li> </ul> </li> </ul> <p>Notably, configuration files can also contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated when launching a pipeline by using the <code>-profile</code> command option:</p> <pre><code>nextflow run nf-core/&lt;workflow&gt; -profile &lt;profile&gt;\n</code></pre> <p>Profiles used by nf-core pipelines include:</p> <ul> <li>Software management profiles<ul> <li>Profiles for the management of software using software management tools, e.g., <code>docker</code>, <code>singularity</code>, and <code>conda</code>.</li> </ul> </li> <li>Test profiles<ul> <li>Profiles to execute the pipeline with a standardised set of test data and parameters, e.g., <code>test</code> and <code>test_full</code>.</li> </ul> </li> </ul> <p>Multiple profiles can be specified in a comma-separated (<code>,</code>) list when you execute your command. The order of profiles is important as they will be read from left to right:</p> <pre><code>nextflow run nf-core/&lt;workflow&gt; -profile test,singularity\n</code></pre> <p>nf-core pipelines are required to define software containers and conda environments that can be activated using profiles. Although it is possible to run the pipelines with software installed by other methods (e.g., environment modules or manual installation), using Docker or Singularity is more convenient and more reproducible.</p> <p>Tip</p> <p>If you're computer has internet access and one of Conda, Singularity, or Docker installed, you should be able to run any nf-core pipeline with the <code>test</code> profile and the respective software management profile 'out of the box'.</p> <p>The <code>test</code> data profile will pull small test files directly from the <code>nf-core/test-data</code> GitHub repository and run it on your local system. The <code>test</code> profile is an important control to check the pipeline is working as expected and is a great way to trial a pipeline. Some pipelines have multiple test <code>profiles</code> for you to try.</p>"},{"location":"session_1/3_configuration/#shared-configuration-files","title":"Shared configuration files","text":"<p>An <code>includeConfig</code> statement in the <code>nextflow.config</code> file is also used to include custom institutional profiles that have been submitted to the nf-core config repository. At run time, nf-core pipelines will fetch these configuration profiles from the nf-core config repository and make them available.</p> <p>For shared resources such as an HPC cluster, you may consider developing a shared institutional profile. You can follow this tutorial for more help.</p>"},{"location":"session_1/3_configuration/#custom-configuration-files","title":"Custom configuration files","text":"<p>Nextflow will also look for files that are external to the pipeline project directory. These files include:</p> <ul> <li>The config file <code>$HOME/.nextflow/config</code></li> <li>A config file named <code>nextflow.config</code> in your current directory</li> <li>Custom files specified using the command line<ul> <li>A parameter file that is provided using the <code>-params-file</code> option</li> <li>A config file that are provided using the <code>-c</code> option</li> </ul> </li> </ul> <p>You don't need to use all of these files to execute your pipeline.</p> <p>Parameter files</p> <p>Parameter files are <code>.json</code> files that can contain an unlimited number of parameters:</p> my-params.json<pre><code>{\n   \"&lt;parameter1_name&gt;\": 1,\n   \"&lt;parameter2_name&gt;\": \"&lt;string&gt;\",\n   \"&lt;parameter3_name&gt;\": true\n}\n</code></pre> <p>You can override default parameters by creating a custom <code>.json</code> file and passing it as a command-line argument using the <code>-param-file</code> option.</p> <pre><code>nextflow run nf-core/&lt;workflow&gt; -profile test,singularity -r main -param-file &lt;path/to/params.json&gt;\n</code></pre> <p>Exercise</p> <p>Give the MultiQC report for the <code>christopher-hakkaart/nf-core-demo</code> pipeline the name of your favorite food using the <code>multiqc_title</code> parameter in a parameters file:</p> Solution <p>Create a custom <code>.json</code> file that contains your favourite food, e.g., cheese:</p> my-custom-params.json<pre><code>{\n\"multiqc_title\": \"cheese\"\n}\n</code></pre> <p>Include the custom <code>.json</code> file in your execution command with the <code>-params-file</code> option:</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -params-file my_custom_params.json \n</code></pre> <p>Check that it has been applied:</p> <pre><code>ls results/multiqc/\n</code></pre> <p>Configuration files</p> <p>Configuration files are <code>.config</code> files that can contain various pipeline properties. Custom paths passed in the command-line using the <code>-c</code> option:</p> <pre><code>nextflow run nf-core/&lt;workflow&gt; -profile test,singularity -c &lt;path/to/custom.config&gt;\n</code></pre> <p>Multiple custom <code>.config</code> files can be included at execution by separating them with a comma (<code>,</code>).</p> <p>Custom configuration files follow the same structure as the configuration file included in the pipeline directory.</p> <p>Configuration properties are organised into scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation. For example:</p> custom.config<pre><code>alpha.x  = 1\nalpha.y  = 'string value'\n</code></pre> <p>Is equivalent to:</p> custom.config<pre><code>alpha {\n     x = 1\n     y = 'string value'\n}\n</code></pre> <p>Scopes allow you to quickly configure settings required to deploy a pipeline on different infrastructure using different software management. For example, the <code>executor</code> scope can be used to provide settings for the deployment of a pipeline on a HPC cluster. Similarly, the <code>singularity</code> scope controls how Singularity containers are executed by Nextflow.</p> <p>Multiple scopes can be included in the same <code>.config</code> file using a mix of dot prefixes and curly brackets. A full list of scopes is described in detail here.</p> <p>Exercise</p> <p>Give the MultiQC report for the <code>christopher-hakkaart/nf-core-demo</code> pipeline the name of your favorite color using the <code>multiqc_title</code> parameter in a custom <code>.config</code> file:</p> Solution <p>Create a custom <code>.config</code> file that contains your favourite colour, e.g., blue:</p> custom.config<pre><code>params.multiqc_title = \"blue\"\n</code></pre> <p>Include the custom <code>.config</code> file in your execution command with the <code>-c</code> option:</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -resume -c custom.config \n</code></pre> <p>Check that it has been applied:</p> <pre><code>ls results/multiqc/\n</code></pre> <p>Why did this fail?</p> <p>You can not use the <code>params</code> scope in custom configuration files. Parameters can only be configured using the <code>-params-file</code> option and the command line. While it parameter is listed as a parameter on the <code>STDOUT</code>, it was not applied to the executed command:</p> <pre><code>nextflow log\nnextflow log &lt;run name&gt; -f \"process,script\"\n</code></pre> <p>The <code>process</code> scope allows you to configure pipeline processes and is used extensively to define resources and additional arguments for modules.</p> <p>By default, process resources are allocated in the <code>conf/base.config</code> file using the <code>withLabel</code> selector:</p> base.config<pre><code>process {\n    withLabel: BIG_JOB {\n        cpus = 16\n        memory = 64.GB\n    }\n}\n</code></pre> <p>Similarly, the <code>withName</code> selector enables the configuration of a process by name. By default, module parameters are defined in the <code>conf/modules.config</code> file:</p> modules.config<pre><code>process {\n    withName: MYPROCESS {\n        cpus = 4\n        memory = 8.GB\n    }\n}\n</code></pre> <p>While some tool arguments are included as a part of a module. To make modules sharable across pipelines, most tool arguments are defined in the <code>conf/modules.conf</code> file in the pipeline code under the <code>ext.args</code> entry.</p> <p>Importantly, having these arguments outside of the module also allows them to be customised at runtime.</p> <p></p> <p></p> <p></p> <p>For example, if you were trying to add arguments in the <code>MULTIQC</code> process in the <code>christopher-hakkaart/nf-core-demo</code> pipeline, you could use the process scope:</p> custom.config<pre><code>process {\n    withName : \".*:MULTIQC\" {\n        ext.args   = { \"&lt;your custom parameter&gt;\" }\n\n    }\n</code></pre> <p>However, if a process is used multiple times in the same pipeline, an extended execution path of the module may be required to make it more specific:</p> custom.config<pre><code>process {\n    withName: \"NFCORE_DEMO:DEMO:MULTIQC\" {\n        ext.args = \"&lt;your custom parameter&gt;\"\n    }\n}\n</code></pre> <p>The extended execution path is built from the pipelines, subworkflows, and module used to execute the process.</p> <p>In the example above, the nf-core <code>MULTIQC</code> module, was called by the <code>DEMO</code> pipeline, which was called by the <code>NFCORE_DEMO</code> pipeline in the <code>main.nf</code> file.</p> <p>How to build an extended execution path</p> <p>It can be tricky to evaluate the path used to execute a module. If you are unsure of how to build the path you can copy it from the <code>conf/modules.conf</code> file. How arguments are added to a process can also vary. Be vigilant.</p> <p>Exercise</p> <p>Create a new <code>.config</code> file that uses the <code>process</code> scope to overwrite the <code>args</code> for the <code>MULTIQC</code> process. Change the <code>args</code> to your favourite month of the year, e.g, <code>\"--title \\\"october\\\"\"</code>.</p> <p>In this example, the <code>\\</code> is used to escape the <code>\"</code> in the string. This is required to ensure the string is passed correctly to the <code>MULTIQC</code> module.</p> Solution <p>Make a custom config file that uses the <code>process</code> scope to replace the <code>args</code> for the <code>MULTIQC</code> process:</p> custom.config<pre><code>process {\n    withName: \"NFCORE_DEMO:DEMO:MULTIQC\" {\n        ext.args = \"--title \\\"october\\\"\"\n    }\n}\n</code></pre> <p>Execute your run command again with the custom configuration file:</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -r main -profile test,singularity -resume -c custom.config\n</code></pre> <p>Check that it has been applied:</p> <pre><code>ls results/multiqc/\n</code></pre> <p>Exercise</p> <p>Demonstrate the configuration hierarchy using the <code>christopher-hakkaart/nf-core-demo</code> pipeline by adding a params file (<code>-params-file</code>), and a command line flag (<code>--multiqc_title</code>) to your execution. You can use the files you have already created. Make sure that the <code>--multiqc_title</code> is different to the <code>multiqc_title</code> in your params file and different to the title you have used in the past.</p> Solution <p>Use the <code>.json</code> file you created previously:</p> my-custom-params.json<pre><code>{\n\"multiqc_title\": \"cheese\"\n}\n</code></pre> <p>Execute your command with your params file (<code>-params-file</code>) and a command line flag (<code>--multiqc_title</code>):</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -r main -profile test,singularity -resume -params-file my_custom_params.json --multiqc_title \"cake\"\n</code></pre> <p>In this example, as the command line is at the top of the hierarchy, the <code>multiqc_title</code> will be \"cake\".</p> <p></p> <p>Key points</p> <ul> <li>nf-core pipelines follow a similar structure.</li> <li>nf-core pipelines are configured using multiple configuration sources.</li> <li>Configuration sources are ranked to decide which settings to apply.</li> <li>Pipeline parameters must be passed via the command line (<code>--&lt;parameter&gt;</code>) or Nextflow <code>-params-file</code> option.</li> </ul>"},{"location":"session_1/4_commands/","title":"nf-core tools for users","text":"<p>Objectives</p> <ul> <li>Learn more about nf-core tooling for users.</li> <li>Use <code>nf-core list</code> to view information about nf-core pipelines.</li> <li>Use <code>nf-core download</code> to download a pipeline and it's singularity images.</li> <li>Use <code>nf-core launch</code> to create a parameters file.</li> </ul>"},{"location":"session_1/4_commands/#nf-core-tools","title":"nf-core tools","text":"<p>nf-core tools has additional commands to help users execute pipelines. Although you do not need to use these commands to execute the nf-core pipelines, they can greatly improve and simplify your experience.</p> <p>There are also nf-core tools for developers. However, these will not be covered as a part of this workshop. If you are curious to learn more about these tools you can find more information on the nf-core websites tools page. There are also lots of excellent ByteSize talks on the nf-core YouTube channel.</p>"},{"location":"session_1/4_commands/#nf-core-list","title":"<code>nf-core list</code>","text":"<p>The nf-core <code>list</code> command can be used to print a list of remote nf-core pipelines along with your local pipeline information.</p> <pre><code>nf-core list\n</code></pre> <p>The output shows the latest pipeline version number and when it was released. You will also be shown if and when a pipeline was pulled locally and whether you have the latest version.</p> <p>Keywords can also be supplied to help filter the pipelines based on matches in titles, descriptions, or topics:</p> <pre><code>nf-core list dna\n</code></pre> <p>Options can also be used to sort the pipelines by latest release (<code>-s release</code>, default), when you last pulled a pipeline locally (<code>-s pulled</code>), alphabetically (<code>-s name</code>), or number by the number of GitHub stars (<code>-s stars</code>).</p> <p>Exercise</p> <p>Filter the list of nf-core pipelines for those that are for <code>dna</code> and sort them by stars. Which <code>dna</code> pipeline has the most stars?</p> Solution <p>Execute the <code>list</code> command, filter it for <code>dna</code>, and sort by <code>stars</code>:</p> <pre><code>nf-core list dna -s stars\n</code></pre>"},{"location":"session_1/4_commands/#nf-core-launch","title":"<code>nf-core launch</code>","text":"<p>A pipeline can have a large number of optional parameters. To help with this, the <code>nf-core launch</code> command is designed to help you write parameter files for when you launch your pipeline.</p> <p>The nf-core <code>launch</code> command takes one argument - either the name of an nf-core pipeline which will be pulled automatically or the path to a directory containing a Nextflow pipeline:</p> <pre><code>nf-core launch nf-core/&lt;pipeline&gt;\n</code></pre> <p>When running this command, you will first be asked about which version of the pipeline you would like to execute. Next, you will be given the choice between a web-based graphical interface or an interactive command-line wizard tool to enter the pipeline parameters. Both interfaces show documentation alongside each parameter, will generate a run ID, and will validate your inputs.</p> <p>The nf-core <code>launch</code> tool uses the <code>nextflow_schema.json</code> file from a pipeline to give parameter descriptions, defaults, and grouping. If no file for the pipeline is found, one will be automatically generated at runtime.</p> <p>The <code>launch</code> tool will save your parameter variables as a <code>.json</code> file called <code>nf-params.json</code>. It will also suggest an execution command that includes the <code>-params-file</code> flag and your new <code>nf-params.json</code> file.</p> <p>The command line wizard will finish by asking if you want to launch the pipeline. Any profiles or options that were set using the wizard will be included in your <code>run</code> command.</p> <p>Exercise</p> <p>Use <code>nf-core launch</code> to launch the the <code>christopher-hakkaart/nf-core-demo</code> pipeline. Use the <code>test</code> and <code>singularity</code> profiles and name your output folder <code>my_test_output</code>.</p> Solution <p>Use the nf-core <code>launch</code> command for the <code>christopher-hakkaart/nf-core-demo</code> pipeline. Your <code>nf-params.json</code> file should look like this:</p> <pre><code>{\n    \"outdir\": \"my_test_output\"\n}\n</code></pre> <p>Your final <code>run</code> command should look like this:</p> <pre><code>nextflow run christopher-hakkaart/nf-core-demo -r main -profile test,singularity -params-file nf-params.json\n</code></pre> <p>The launch website</p> <p>You can also use the launch command directly from the nf-core launch website. In this case, you can configure your pipeline using the wizard and then copy the outputs to your terminal or use the run id generated by the wizard. You will need to be connected to the internet to use the run id.</p> <pre><code>nf-core launch --id &lt;run_id&gt;\n</code></pre>"},{"location":"session_1/4_commands/#nf-core-download","title":"<code>nf-core download</code>","text":"<p>Sometimes you may need to execute an nf-core pipeline on a server or HPC system that has no internet connection. In this case, you will need to fetch the pipeline files and manually transfer them to your offline system. To make this process easier and ensure accurate retrieval of correctly versioned code and software containers, nf-core has the <code>download</code> command.</p> <p>The nf-core <code>download</code> command will download both the pipeline code and the institutional nf-core/configs files. It can also optionally download singularity image file.</p> <pre><code>nf-core download\n</code></pre> <p>If run without any arguments, the download tool will interactively prompt you for the required information. Each prompt option has a flag and if all flags are supplied then it will run without a request for any additional user input:</p> <ul> <li>Pipeline name<ul> <li>Name of pipeline you would like to download.</li> </ul> </li> <li>Pipeline revision<ul> <li>The revision you would like to download.</li> </ul> </li> <li>Pull containers<ul> <li>If you would like to download Singularity images.</li> <li>The path to a folder where you would like to store these images if you have not set your <code>NXF_SINGULARITY_CACHEDIR</code>.</li> </ul> </li> <li>Choose compression type<ul> <li>The compression type for Singularity images.</li> </ul> </li> </ul> <p>Alternatively, you could build your own execution command with the command line options.</p> <p>Exercise</p> <p>Use the nf-core <code>download</code> command to download the <code>christopher-hakkaart/nf-core-demo</code> pipeline with it's uncompressed Singularity images.</p> Solution <p>Use the nf-core <code>download</code> command for the <code>christopher-hakkaart/nf-core-demo</code> pipeline and follow the prompts.</p> <p>Your output should look like this:</p> <pre><code>INFO Saving 'christopher-hakkaart/nf-core-demo'                                                                                                                               \n  Pipeline revision: 'main'                                                                                                                                               \n  Use containers: 'singularity'                                                                                                                                           \n  Container library: 'quay.io'                                                                                                                                            \n  Using $NXF_SINGULARITY_CACHEDIR': /Users/chrishakkaart/tools/singularity'                                                                                               \n  Output directory: 'christopher-hakkaart-nf-core-demo_main'                                                                                                              \n  Include default institutional configuration: 'False'    \n</code></pre> <p></p> <p>Key points</p> <ul> <li>The nf-core <code>list</code> command can be used to view local and remote information about nf-core pipelines</li> <li>The nf-core <code>launch</code> command can be a useful tool for writing parameter files</li> <li>The nf-core <code>download</code> command is a powerful way to download a pipeline and its Singularity images</li> </ul>"},{"location":"session_2/0_kickoff/","title":"Introduction to Session 2","text":"<p>Session 2 builds on fundamental concepts learned in Session 1 and provides you with hands-on experience in nf-core pipeline customisation.</p> <p>We will explore the pipeline source code and apply various customisations using a parameter file and custom configuration files. You will:</p> <ul> <li>Explore the nf-core/sarek source code</li> <li>Create custom input files and customise the pipeline execution</li> <li>Review the execution of the pipeline with the customisations applied</li> </ul> <p>Each lesson in this session will build on previous lessons. By the end of this session you will have a deeper understanding of the customisation techniques and the impact they have on the pipeline execution.</p> <p>Applying what you learn</p> <p>Although activities in this session will use version 3.2.3 of the nf-core/sarek pipeline, all customisation scenarios we explore are applicable to other nf-core pipelines.</p>"},{"location":"session_2/0_kickoff/#log-back-in-into-nesi","title":"Log back in into NESI","text":"<p>Follow set up instructions to log back into Nesi.</p>"},{"location":"session_2/0_kickoff/#load-modules-and-activate-the-nf-core-conda-environment","title":"Load modules and activate the nf-core conda environment","text":"<p>If your previous session has ended you may need to load the required modules and activate the nf-core conda environment again by running the following <code>source</code> command</p> <pre><code>source /nesi/project/nesi02659/nextflow-workshop/init-nf-day2\n</code></pre>"},{"location":"session_2/0_kickoff/#create-a-new-work-directory","title":"Create a new work directory","text":"<p>Create a new directory for all session 2 activities and move into it:</p> <pre><code>mkdir ~/session2 &amp;&amp; cd $_\n</code></pre>"},{"location":"session_2/1_sarek/","title":"nf-core/sarek","text":"<p>Objectives</p> <ul> <li>Understand the Sarek pipeline structure and default usage</li> <li>Understand the levels of customisation available for nf-core pipelines</li> <li>Use the nf-core documentation to select appropriate parameters for a run command </li> <li>Write and run a nf-core sarek command on the command line </li> <li>Explore pipeline deployment and outputs </li> </ul>"},{"location":"session_2/1_sarek/#the-sarek-pipeline","title":"The Sarek pipeline","text":"<p>nf-core/sarek is a pipeline designed to detect variants on whole genome or targeted sequencing data. Initially designed for Human, and Mouse, it can work on any species with a reference genome. Sarek can also handle tumour/normal pairs and could include additional relapses.</p> <p>The pipeline makes use of Docker/Singularity containers, making installation trivial and results highly reproducible. The Nextflow DSL2 implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. </p> <p>Depending on the options and samples provided, the pipeline can currently perform the following:</p> <ul> <li>Form consensus reads from UMI sequences (<code>fgbio</code>)</li> <li>Sequencing quality control and trimming (<code>FastQC</code>, <code>fastp</code>)</li> <li>Map Reads to Reference (<code>BWA-mem</code> or <code>BWA-mem2</code> or <code>dragmap</code>)</li> <li>Process BAM file (<code>GATK MarkDuplicates</code>, <code>GATK BaseRecalibrator</code>, <code>GATK ApplyBQSR</code>)</li> <li>Summarise alignment statistics (<code>samtools stats</code>, <code>mosdepth</code>)</li> <li>Variant calling (enabled by <code>--tools</code>, see compatibility):<ul> <li><code>HaplotypeCaller</code>, <code>freebayes</code>, <code>mpileup</code>, <code>Strelka2</code>, <code>DeepVariant</code>, <code>Mutect2</code>, <code>Manta</code>, <code>TIDDIT</code>, <code>ASCAT</code>, <code>Control-FREEC</code>, <code>CNVkit</code>, and / or <code>MSIsensor-pro</code></li> </ul> </li> <li>Variant filtering and annotation (<code>SnpEff</code>, <code>Ensembl VEP</code>)</li> <li>Summarise and represent QC (<code>MultiQC</code>)</li> </ul> <p>nf-core pipelines are frequently represented as subway maps. The nf-core/sarek subway map is shown below and is a good place to start when first understanding how the pipeline works.</p> <p></p> <p></p> <p></p>"},{"location":"session_2/1_sarek/#download-the-sarek-pipeline","title":"Download the Sarek pipeline","text":"<p>There are multiple ways you can download and store a copy of a nf-core pipeline.</p> <p>Firstly, you could use the <code>nextflow pull</code> command. By default, if you you the <code>nextflow run</code> command to execute a pipeline from github it will also pull the pipeline. In both of these cases the pipeline will be stored in a hidden directory in your home directory.</p> <p>Secondly, you could clone a copy of the pipeline using the standard <code>git clone</code> command, e.g., <code>git clone https://github.com/nf-core/sarek.git</code>. This will download the pipeline to your current working directory.</p> <p>Finally, you could use the <code>nf-core download</code> utility to download a copy of the pipeline. This will give the the option to download the pipeline code, the required singularity images, and the institutional configs from the nf-core github repository. This method can be especially helpful if you are working offline and want to move all of the pipeline code and tooling to a different machine.</p>"},{"location":"session_2/1_sarek/#getting-started","title":"Getting started","text":"<p>All nf-core pipelines are provided with comprehensive documentation that explain what the default pipeline structure entails and options for customising this based on your needs. It is important to remember that nf-core pipelines typically do not include all possible tool parameters. Instead, they provide a sensible set of parameters that are suitable for most use cases.</p> <p>The number and type of parameters an nf-core pipeline accepts differ between pipelines. The recommended (typical) run command and all the parameters available for the nf-core/sarek pipeline can be viewed using the <code>--help</code> flag:</p> <pre><code>nextflow run nf-core/sarek -r 3.2.3 --help \n</code></pre> <p>Revision 3.2.3</p> <p>The Sarek pipeline is always improving but we want to ensure that the results of this workshop are reproducible. To ensure this, we will use a specific version (3.2.3) of the pipeline and the revision flag (<code>-r</code>).</p> <p>At the top of the help output, you will see the recommended run command:</p> <pre><code>nextflow run nf-core/sarek --input samplesheet.csv --genome GATK.GRCh38 -profile docker\n</code></pre> <p>It outlines the requirement for three things: </p> <ul> <li>An input samplesheet (<code>--input</code>)</li> <li>A reference genome (<code>--genome</code>) </li> <li>A software management profile (<code>--profile</code>)</li> </ul> <p>Hyphens matter</p> <p>Nextflow-specific parameters use one (<code>-</code>) hyphen, whereas pipeline-specific parameters use two (<code>--</code>).</p>"},{"location":"session_2/1_sarek/#more-information-about-sarek","title":"More information about Sarek","text":"<p>There is extensive information about nf-core pipelines on the nf-core website. The dedicated Sarek pipeline page is the best resource for information about the pipeline and how to execute it.</p> <p>If you have specific questions that are not included in the documentation you can join the nf-core Slack workspace and ask in the <code>#sarek</code> channel.</p>"},{"location":"session_2/1_sarek/#testing-a-pipeline","title":"Testing a pipeline","text":"<p>Before running a pipeline on your own data, it is a good idea to test the pipeline on a small dataset. This allows you to check that the pipeline is working as expected without having to wait for a long time for the pipeline to complete.</p> <p>The <code>test</code> profile will run the pipeline on a small test dataset that is included with the pipeline code. </p> <pre><code>nextflow run nf-core/sarek -profile test,singularity --outdir test_sarek -r 3.2.3\n</code></pre> <p>Exercise</p> <p>Check that Sarek is working by running the pipeline with the <code>test</code> profile. </p> Solution <p>Run the test profile:</p> <pre><code>nextflow run nf-core/sarek -profile test,singularity --outdir test_sarek -r 3.2.3\n</code></pre> <p>Test data</p> <p>The <code>--input</code> and <code>--genome</code> parameters are not required when using the <code>test</code> profile. The test data and small reference files are included with the pipeline code and are automatically used when the <code>test</code> profile is specified. </p> <p></p> <p>Key points</p> <ul> <li>nf-core pipelines are provided with sensible default settings and required inputs.</li> <li>The <code>--help</code> flag can be used to view the recommended run command and all available parameters.</li> <li>The <code>test</code> profile can be used to show that a pipeline is working as expected.</li> </ul>"},{"location":"session_2/2_runcommand/","title":"Configuring your run","text":"<p>Objectives</p> <ul> <li>Understand the different parts of a run command</li> <li>Learn how to customise a run command</li> <li>Learn how to use a parameter file and configuration files to customise a run command</li> </ul>"},{"location":"session_2/2_runcommand/#where-to-start","title":"Where to start","text":"<p>A recommended run command can be found for each pipeline on the nf-core website and is a useful starting point for customising a run command:</p> <pre><code>nextflow run nf-core/sarek --input samplesheet.csv --genome GATK.GRCh38 -profile docker\n</code></pre> <p>From here, the command can be customised to suit your needs. The following sections will describe the different components of the command and how they can be customised.</p> <p>In this example, the same small test data files that are used in the test profile will be used to demonstrate how to create you own sample sheet. However, you will be writing the files rather than relying on the test profile. The same concepts will apply to data on your local storage.</p>"},{"location":"session_2/2_runcommand/#input-input","title":"Input (<code>--input</code>)","text":"<p>The Sarek pipeline requires a samplesheet as an input parameter. This is a <code>csv</code> file that contains information about the samples that will be processed. The samplesheet is used to specify the location of the input data, the sample name, and additional metadata.</p> <p>A samplesheet is created manually and can be stored anywhere on your computer. The samplesheet can be named anything you like, but it must be specified using the <code>--input</code> flag.</p> <p>More information about how to structure a samplesheet for Sarek can be found in the usage documentation.</p> <p>Note how Sarek can accept different data types as inputs and how the samplesheet is different for each.</p> <p>Exercise</p> <p>Use the usage documentation to create <code>samplesheet.csv</code> in your working directory. It must include the following data in the required format:</p> <ul> <li>patient: test</li> <li>sex: XX</li> <li>status: 0</li> <li>sample: test</li> <li>lane: test_L1</li> <li>fastq_1: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_1.fastq.gz</li> <li>fastq_2: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_2.fastq.gz</li> </ul> Solution <p>Your <code>csv</code> file should look like the following:</p> samplesheet.csv<pre><code>patient,sex,status,sample,lane,fastq_1,fastq_2\ntest,XX,0,test,test_L1,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_2.fastq.gz\n</code></pre>"},{"location":"session_2/2_runcommand/#reference-data-genome","title":"Reference data (<code>--genome</code>)","text":"<p>Many nf-core pipelines need a reference genome for alignment, annotation, or similar.</p> <p>To make the use of reference genomes easier, Illumina developed a centralised resource called iGenomes where the most commonly used reference genome files are organised in a consistent structure.</p> <p>nf-core have uploaded a copy of iGenomes onto AWS S3 and nf-core pipelines are configured to use this by default. All AWS iGenomes paths are specified in pipelines that support them in <code>conf/igenomes.config</code>. By default, the pipeline will automatically download the required reference files when you it is executed with an appropriate genome key (e.g., <code>--genome GRCh37</code>). The pipeline will only download what it requires.</p> <p>Downloading reference genome files takes time and bandwidth so, if possible, it is recommend that you download a local copy of your relevant iGenomes references and configure your execution to use the local version.</p> <p>When executing Sarek with common genomes, such as GRCh38 and GRCh37, iGenomes is shipped with the necessary reference files. However, depending on your deployment, it is sometimes necessary to use custom references for some or all files. Specific details for different deployment situations are described in the usage documentation.</p> <p>The small test <code>fastq.gz</code> files in the samplesheet created above would throw errors if it was run with a full size reference genome from iGenomes. Instead, smaller files from the nf-core test datasets repository need to be used. As these files are not included in iGenomes they must be specified manually using parameters.</p> <p>The following parameters can be used to specify the required reference/annotation files for the small test data files:</p> <pre><code>    --igenomes_ignore \\\n    --dbsnp \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\" \\\n    --fasta \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\" \\\n    --germline_resource \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\" \\\n    --intervals \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\" \\\n    --known_indels \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\" \\\n    --snpeff_db 105 \\\n    --snpeff_genome \"WBcel235\" \\\n    --snpeff_version \"5.1\" \\\n    --vep_cache_version \"106\" \\\n    --vep_genome \"WBcel235\" \\\n    --vep_species \"caenorhabditis_elegans\" \\\n    --vep_version \"106.1\" \\\n    --max_cpus 2 \\\n    --max_memory 6.5GB \\\n    --tools \"freebayes\" \\\n    --outdir \"my_results\"\n</code></pre> <p>Tools</p> <p>The <code>--tools</code> parameter is included above to trigger the execution of the freebayes variant caller. Multiple variant callers are available as a part of Sarek, however, in this example, only one is included.</p> <p>Warning</p> <p>The <code>--igenomes_ignore</code> parameter must be included when using custom reference/annotation files. Without it, by default, the reference/annotation files that are typically required by Sarek are downloaded for <code>GATK.GRCh38</code>. Some pipelines have this feature by default while others require the genome flag (or alternate) for every execution.</p>"},{"location":"session_2/2_runcommand/#profiles-profile","title":"Profiles (<code>--profile</code>)","text":"<p>Software profiles are used to specify the software environment in which the pipeline will be executed. By simply including a profile (e.g., <code>singularity</code>), Nextflow will download, store, and manage the software used in the Sarek pipeline.</p> <p>To ensure reproducibility, it is recommended that you use container technology, e.g., <code>docker</code> or <code>singularity</code>. </p>"},{"location":"session_2/2_runcommand/#putting-it-all-together","title":"Putting it all together","text":"<p>The previous sections have shown different parts of a recommended run command. These can be combined to create a custom run command that will execute Sarek on the small test data files.</p> <p>The completed run command will execute a small test set of files using the freebayes variant caller. The command will use the small test data files from the nf-core test datasets repository and custom reference/annotation files that are hosted on github.</p> <p>Exercise</p> <p>Use all of the information above to build a custom run command that will execute version 3.2.3 of Sarek on the samplesheet you created.</p> Hint <p>Include the sample sheet you made earlier with <code>--input samplesheet.csv</code>. Remember is must be in your working directory or you must specify the full or relative path to the file. If you named your samplesheet differently it must be reflected in your command.</p> <p>Include the parameters shown above to specify the reference/annotation files. You can copy these directly into your run command.</p> <p>Use Singularity to manage your software with <code>-profile singularity</code>.</p> Solution <p>Your run command should look like the following:</p> <pre><code>nextflow run nf-core/sarek \\\n    --input samplesheet.csv \\\n    --igenomes_ignore \\\n    --dbsnp \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\" \\\n    --fasta \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\" \\\n    --germline_resource \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\" \\\n    --intervals \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\" \\\n    --known_indels \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\" \\\n    --snpeff_db 105 \\\n    --snpeff_genome \"WBcel235\" \\\n    --snpeff_version \"5.1\" \\\n    --vep_cache_version \"106\" \\\n    --vep_genome \"WBcel235\" \\\n    --vep_species \"caenorhabditis_elegans\" \\\n    --vep_version \"106.1\" \\\n    --max_cpus 2 \\\n    --max_memory 6.5GB \\\n    --tools \"freebayes\" \\\n    --outdir \"my_results\" \\\n    -profile singularity \\\n    -r 3.2.3\n</code></pre> <p>If everything has worked - you will see the pipeline launching in your terminal \ud83d\ude80</p>"},{"location":"session_2/2_runcommand/#customizing-parameters","title":"Customizing parameters","text":"<p>In Session 1, we learned how to customise a simple nf-core pipeline. Here, we will apply these skills to customise the execution of the Sarek pipeline.</p> <p>In the previous section we supplied a series of pipeline parameters as flags in your run command (<code>--</code>). Here, we will package these into a <code>.json</code> file and use the <code>-params-file</code> option.</p> <p>Exercise</p> <p>Package the parameters from the previous lesson into a <code>.json</code> file and run the pipeline using the <code>-params-file</code> option:</p> <pre><code>nextflow run nf-core/sarek \\\n    --input samplesheet.csv \\\n    --igenomes_ignore \\\n    --dbsnp \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\" \\\n    --fasta \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\" \\\n    --germline_resource \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\" \\\n    --intervals \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\" \\\n    --known_indels \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\" \\\n    --snpeff_db 105 \\\n    --snpeff_genome \"WBcel235\" \\\n    --snpeff_version \"5.1\" \\\n    --tools \"freebayes\" \\\n    --vep_cache_version \"106\" \\\n    --vep_genome \"WBcel235\" \\\n    --vep_species \"caenorhabditis_elegans\" \\\n    --vep_version \"106.1\" \\\n    --max_cpus 4 \\\n    --max_memory 6.5GB \\\n    --output \"my_results\"\n    -profile singularity \\\n    -r 3.2.3\n</code></pre> Solution my-params.json<pre><code>{\n    \"igenomes_ignore\": true,\n    \"dbsnp\": \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\",\n    \"fasta\": \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\",\n    \"germline_resource\": \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\",\n    \"intervals\": \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\",\n    \"known_indels\": \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\",\n    \"snpeff_db\": 105,\n    \"snpeff_genome\": \"WBcel235\",\n    \"snpeff_version\": \"5.1\",\n    \"tools\":  \"freebayes\", \n    \"vep_cache_version\": 106,\n    \"vep_genome\": \"WBcel235\",\n    \"vep_species\": \"caenorhabditis_elegans\",\n    \"vep_version\": \"106.1\",\n    \"max_cpus\": 4,\n    \"max_memory\": \"6.5 GB\",\n    \"outdir\": \"my_results_2\"\n}\n</code></pre> <p>Your execution command will now look like this:</p> <pre><code>nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3.2.3\n</code></pre> <p>Note that in this example we kept <code>--input samplesheet.csv</code> in the execution command. However, this could have put this in the <code>.json</code> file. You can pick and choose which parameters go in a params file and which parameters go in your execution command.</p> <p>Due to the order of priority, you can modify parameters you want to change without having to edit your newly created parameters file.</p> <p>Exercise</p> <p>Include both <code>freebayes</code> and <code>strelka</code> as variant callers using the <code>tools</code> parameter and run the pipeline again.</p> <p>For this option, you will need to use the <code>--tools</code> flag and include both variant callers in the same string separated by a comma, e.g., <code>--tools \"&lt;tool1&gt;,&lt;tool2&gt;\"</code></p> <p>You can also use <code>-resume</code> to resume the pipeline from the last successful step.</p> Solution <pre><code>nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3.2.3 --tools \"freebayes,strelka\" -resume \n</code></pre>"},{"location":"session_2/2_runcommand/#configuration-files","title":"Configuration files","text":"<p>Sometimes Sarek won't have a parameter that you need to customize the execution of a tool. In this situation, you will need to apply a configuration file to the pipeline.</p> <p>As shown in the example from Session 1, you can selectively apply a configuration file to a process using the <code>withName</code> directive.</p> <p>Be specific with your selector</p> <p>Remember to make your selector specific to the process you are trying to customise. It can be helpful to use the same selectors that are already included in the configuration file.</p> <p>Sarek is a little different from other pipelines because it has multiple different config files that are used for different tools or groups of tools. This is stylistic and helps to keep the config files organised.</p> <p>For example, there are a number of options that are applied when calling variants with <code>FREEBAYES</code> and are all stored in a freebayes.config file together:</p> freebayes.config<pre><code>process {\n\n    withName: 'MERGE_FREEBAYES' {\n        ext.prefix       = { \"${meta.id}.freebayes\" }\n        publishDir       = [\n            mode: params.publish_dir_mode,\n            path: { \"${params.outdir}/variant_calling/freebayes/${meta.id}/\" },\n            saveAs: { filename -&gt; filename.equals('versions.yml') ? null : filename }\n        ]\n    }\n\n    withName: 'FREEBAYES' {\n        ext.args         = '--min-alternate-fraction 0.1 --min-mapping-quality 1'\n        //To make sure no naming conflicts ensure with module BCFTOOLS_SORT &amp; the naming being correct in the output folder\n        ext.prefix       = { meta.num_intervals &lt;= 1 ? \"${meta.id}\" : \"${meta.id}.${target_bed.simpleName}\" }\n        ext.when         = { params.tools &amp;&amp; params.tools.split(',').contains('freebayes') }\n        publishDir       = [\n            enabled: false\n        ]\n    }\n\n    withName: 'BCFTOOLS_SORT' {\n        ext.prefix       = { meta.num_intervals &lt;= 1 ? meta.id + \".freebayes\" : vcf.name - \".vcf\" + \".sort\" }\n        publishDir       = [\n            mode: params.publish_dir_mode,\n            path: { \"${params.outdir}/variant_calling/\" },\n            pattern: \"*vcf.gz\",\n            saveAs: { meta.num_intervals &gt; 1 ? null : \"freebayes/${meta.id}/${it}\" }\n        ]\n    }\n\n    withName : 'TABIX_VC_FREEBAYES' {\n        publishDir       = [\n            mode: params.publish_dir_mode,\n            path: { \"${params.outdir}/variant_calling/freebayes/${meta.id}/\" },\n            saveAs: { filename -&gt; filename.equals('versions.yml') ? null : filename }\n        ]\n    }\n\n    // PAIR_VARIANT_CALLING\n    if (params.tools &amp;&amp; params.tools.split(',').contains('freebayes')) {\n        withName: '.*:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_FREEBAYES:FREEBAYES' {\n            ext.args       = \"--pooled-continuous \\\n                            --pooled-discrete \\\n                            --genotype-qualities \\\n                            --report-genotype-likelihood-max \\\n                            --allele-balance-priors-off \\\n                            --min-alternate-fraction 0.03 \\\n                            --min-repeat-entropy 1 \\\n                            --min-alternate-count 2 \"\n        }\n    }\n}\n</code></pre> <p>Each of these can be modified independently of the others and be applied using a custom configuration file.</p> <p>Exercise</p> <p>Create a custom configuration file that will modify the <code>--min-alternate-fraction</code> parameter for <code>FREEBAYES</code> to <code>0.05</code> and apply it to the pipeline.</p> Solution <pre><code>nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3.2.3 --tools \"freebayes,strelka\" -resume -c my-config.config\n</code></pre> my-config.config<pre><code>process {\n\n    withName: 'FREEBAYES' {\n        ext.args         = '--min-alternate-fraction 0.05'\n    }\n}\n</code></pre>"},{"location":"session_2/2_runcommand/#metrics-and-reports","title":"Metrics and reports","text":"<p>Nextflow can also produce multiple reports and charts that show several runtime metrics and your execution information. You can enable this functionality by adding Nextflow options to your run command:</p> <ul> <li>Adding <code>-with-report</code> to your run command will create a HTML execution report which includes many useful metrics about a pipeline execution. </li> <li>Adding <code>-with-trace</code> option to creates an execution tracing file that contains some useful information about each process executed in your pipeline script.</li> <li>Adding <code>-with-timeline</code> to your run command enables the creation of the pipeline timeline report showing how processes were executed over time.</li> <li>Adding <code>-with-dag</code> to your run command enables the rendering of the pipeline execution direct acyclic graph representation.<ul> <li>This feature requires the installation of Graphviz on your computer. Beginning in version 22.04, Nextflow can render the DAG as a Mermaid diagram. Mermaid diagrams are particularly useful because they can be embedded in GitHub Flavored Markdown without having to render them yourself.</li> </ul> </li> </ul> <p>Note</p> <p>The execution report (<code>-with-report</code>), trace report (<code>-with-trace</code>), timeline trace (<code>-with-timeline</code>), and dag (<code>-with-dag</code>) must be specified when the pipeline is executed. By contrast, the <code>log</code> option is useful after a pipeline has already run and is available for every executed pipeline.</p> <p>Exercise</p> <p>Try to run the following command and view the reports generated by Nextflow:</p> <pre><code>nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3.2.3 --tools \"freebayes,strelka\" -with-report -with-trace -with-timeline -with-dag -resume\n</code></pre> <p></p> <p>Key points</p> <ul> <li>Sarek comes with a test profiles that can be used to test the pipeline on your infrastructure</li> <li>Sample sheets are <code>csv</code> files that contain important meta data and the paths to your files</li> <li>Reference files are available from iGenomes</li> <li>Parameters go in parameters files and everything else goes in a configuration file</li> <li>There are additional flags you can use to generate reports and metric for you own records</li> </ul>"},{"location":"session_2/3_hpc/","title":"Configuring your deployment","text":"<p>Objectives</p> <ul> <li>Learn ways to configure your deployment using an executor</li> <li>Learn how to configure resources for your pipeline</li> <li>Learn about nf-core configs</li> </ul>"},{"location":"session_2/3_hpc/#choosing-your-executor","title":"Choosing your executor","text":"<p>It is likely that you will also want to configure more than just the pipeline tools. Nextflow has many other scopes which allow you to configure the deployment of your pipeline, as well as give you additional control over how it is being executed.</p> <p>By default your executor will be set to <code>local</code>. However, Nextflow supports all of the major HPC and cloud providers and you can quickly and easily change where your pipeline is run using the <code>executor</code> scope.</p> <p>For example, if you would like to use the <code>slurm</code> executor. This can be done by adding the following to a config file:</p> custom.config<pre><code>executor {\n    name = 'slurm'\n}\n</code></pre> <p>Exercise</p> <p>Try to run the <code>christopher-hakkaart/nf-core-demo</code> pipeline using the <code>-c</code> flag to specify a custom config file with the code above: </p> Solution <pre><code>nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -c custom.config\n</code></pre> <p>Check your job has been submitted using the <code>squeue</code> command:</p> <pre><code>squeue --me\n</code></pre>"},{"location":"session_2/3_hpc/#configuring-your-executor","title":"Configuring your executor","text":"<p>The possibilities for your configuration execution and your executor are endless. As the executor you will need to be mindful of the resources you are requesting and the resources available on your cluster. The many different ways nextflow can configure can be configured to help manage this.</p> <p>For example, you can also configure the management of your job submissions, e.g., setting the queue size to avoid flooding the cluster:</p> custom.config<pre><code>executor {\n    name = 'slurm'\n    queueSize = 4\n}\n</code></pre> <p>Similarly, you might want to control the resources requested by your pipeline. For HPC's, you can't use the <code>cpu</code> and <code>memory</code> parameters as these are set by the scheduler. Instead, you can use parameters defined by nf-core pipelines to set limits to stop a task requesting more resources than what is available.</p> custom.config<pre><code>executor {\n    name = 'slurm'\n    queueSize = 4\n}\n\nparams {\n  max_memory = 8.GB\n  max_cpus = 4\n  max_time = 1.h\n}\n</code></pre> <p>If the pipeline is requesting too much, you can modify the resource allocation of specific processes or label using process selectors.</p> custom.config<pre><code>executor {\n    name = 'slurm'\n    queueSize = 4\n}\n\nparams {\n  max_memory = 8.GB\n  max_cpus = 4\n  max_time = 1.h\n}\n\nprocess {\n    withName: \"NFCORE_DEMO:DEMO:MULTIQC\" {\n        cpus = 4\n    }\n\n    withLabel:process_low {\n        cpus   = 2\n    }\n}\n</code></pre>"},{"location":"session_2/3_hpc/#mixing-executors","title":"Mixing executors","text":"<p>Another scenario might be that you only want to send specific jobs to your cluster, or a different.</p> custom.config<pre><code>executor {\n    name = 'local'\n}\n\nparams {\n  max_memory = 8.GB\n  max_cpus = 4\n  max_time = 1.h\n}\n\nprocess {\n    withName: \"NFCORE_DEMO:DEMO:MULTIQC\" {\n        executor = 'slurm'\n        cpus = 4\n    }\n\n    withLabel:process_low {\n        cpus   = 2\n    }\n}\n</code></pre> <p>The examples above are simple ways you can configure your pipeline to run on your cluster. However, there are many more options available to you. For more information, see the Nextflow documentation.</p>"},{"location":"session_2/3_hpc/#nf-coreconfigs","title":"nf-core/configs","text":"<p>Many nf-core users create an institutional profile and submit it in the nf-core/configs repository. These profiles can be used by anyone and are a great way to share your customisations with the community.</p> <p>The nf-core/configs is loaded by default by nf-core pipelines, meaning that you can apply your customisations by simply adding the <code>-profile &lt;your_profile&gt;</code> flag to your run command.</p> <p>If you are preparing a config for your group or institution, please consider submitting it to [nf-core/configs] to help others in the community.</p> <p></p> <p>Key points</p> <ul> <li>Sarek comes with a test profiles that can be used to test the pipeline on your infrastructure</li> <li>Sample sheets are <code>csv</code> files that contain important meta data and the paths to your files</li> <li>Reference files are available from iGenomes</li> <li>Parameters go in parameters files and everything else goes in a configuration file</li> </ul>"},{"location":"setup/setup/","title":"Log into Nesi Jupyter","text":"<p>During this workshop we will be running the material on the NeSI platform, using the Jupyter interface, however it is also possible to run this material locally on your own machine.</p> <p>One of the differences between running on NeSI or your own machine is that on NeSI we pre-install popular software and make it available to our users, whereas on your own machine you need to install the software yourself (e.g., using a package manager such as conda).</p> Connect to Jupyter on NeSI - Make sure to spawn <code>4CPU</code>, <code>8GB</code> Jupyter sessions <ol> <li>Connect to https://jupyter.nesi.org.nz</li> <li><p>Enter NeSI username, HPC password, and 6 digit second factor token (as set on MyNeSI)</p></li> <li><p>Choose server options as below make sure to choose the correct project code <code>nesi02659</code>, number of CPUs 4, memory 8GB prior to pressing  button.  <li><p>Start a terminal session from the JupyterLab launcher"},{"location":"setup/setup/#loading-required-software","title":"Loading required software","text":"<p>This workshop will use a combination of \"environment modules\" and manually installed software.</p> <p>We will need to prepare our environment by running the following command to source init script which clear the environment, load required software and activate the pre-configured conda environment</p> <pre><code>source /nesi/project/nesi02659/nextflow-workshop/init-nf-day1\n</code></pre> Supplementary - How did we prepare the conda environment <pre><code>module purge\nmodule load Miniconda3\nsource $(conda info --base)/etc/profile.d/conda.sh\n\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n\nexport CONDA_ENVS_PATH=/nesi/project/nesi02659/.conda/envs\n\n#make sure the conda pkgs gets-redirected to nobackkup/scratch space\n#for more information, refer to https://support.nesi.org.nz/hc/en-gb/articles/360001580415-Miniconda3#prevent-conda-from-using-home-storage\nmkdir /nesi/nobackup/nesi02659/conda-pkgs/$USER &amp;&amp; conda config --add pkgs_dirs /nesi/nobackup/nesi02659/conda-pkgs/$USER\n\nconda create --name nf-core python=3.11 nf-core nextflow --solver=libmamba -y\n</code></pre> <p>More details about environment modules can be found on the NeSI support page.</p>"}]}